{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Question:\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "For this assignment, we have selected the dataset from short news articles(https://drive.google.com/open?id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms). This dataset contains a training and testing dataset. Train has around 120K short news articles with the classification, title and description. We need to classify the test document which has around 7.2K short news. \n",
    "\n",
    "As it has four different labels, this problem is multilabel classification. We will also categorize the news with multiple algorithms and generate confidence interval for the predicted class. \n",
    "\n",
    "As a next step will try to create convolutional neural network to classify the news which has different classification labels from various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode, median\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the train and test dataset\n",
    "df = pd.read_csv(\"./data/train.csv\",sep=\",\",header=None)\n",
    "df_test = pd.read_csv(\"./data/test.csv\",sep=\",\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset is huge, we will take 1/6th of the dataset from the train class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(int(df.count()[1]/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50826</th>\n",
       "      <td>4</td>\n",
       "      <td>Intel Pushes Further into Mobile Space (NewsFa...</td>\n",
       "      <td>NewsFactor - Intel has served notice that it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10307</th>\n",
       "      <td>2</td>\n",
       "      <td>Dominating Dirrell dispatches Despaigne</td>\n",
       "      <td>Leon Lawson was watching over Andre Dirrell #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114369</th>\n",
       "      <td>3</td>\n",
       "      <td>Honeywell Agrees Takeover of UK #39;s Novar</td>\n",
       "      <td>US manufacturer Honeywell International agreed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79604</th>\n",
       "      <td>3</td>\n",
       "      <td>Lord James Hanson, prominent British businessm...</td>\n",
       "      <td>Lord James Hanson, a wealthy industrialist who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56353</th>\n",
       "      <td>2</td>\n",
       "      <td>Gallacher #39;s wedge shot wraps up  #39;dream...</td>\n",
       "      <td>A famous name once more adorns a European Tour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                                                  1  \\\n",
       "50826   4  Intel Pushes Further into Mobile Space (NewsFa...   \n",
       "10307   2            Dominating Dirrell dispatches Despaigne   \n",
       "114369  3        Honeywell Agrees Takeover of UK #39;s Novar   \n",
       "79604   3  Lord James Hanson, prominent British businessm...   \n",
       "56353   2  Gallacher #39;s wedge shot wraps up  #39;dream...   \n",
       "\n",
       "                                                        2  \n",
       "50826   NewsFactor - Intel has served notice that it w...  \n",
       "10307   Leon Lawson was watching over Andre Dirrell #3...  \n",
       "114369  US manufacturer Honeywell International agreed...  \n",
       "79604   Lord James Hanson, a wealthy industrialist who...  \n",
       "56353   A famous name once more adorns a European Tour...  "
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Peek of the train input file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the function contains title and description, we will combine it to a single string and its classification label. Also drop unnecessary columns and convert it to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df,only_str = 'N'):\n",
    "    \"\"\"Function for combining and cleaning the text\"\"\"\n",
    "    # Combine heading and news\n",
    "    df['news'] = df[1].map(str) + ' - ' + df[2].map(str)\n",
    "    # Drop previous column\n",
    "    df.drop([1,2],axis=1,inplace=True)\n",
    "    documents = list(df.to_records(index=False))\n",
    "    strings = df['news'].to_string(index=False)\n",
    "    \n",
    "    # Return the format according to type required\n",
    "    if only_str=='N':\n",
    "        return documents\n",
    "    else:\n",
    "        return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intel Pushes Further into Mobile Space (NewsFa...\\n'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combined data for training. All the data is converted to string bytes\n",
    "news = data_cleaning(df,'Y')\n",
    "news[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined data for testing. All the data is converted to records\n",
    "doc = data_cleaning(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to build the features list. This features will contain all the top words sorted by frequency. Once this is list is compiled, we will build the individual features list for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intel',\n",
       " 'Pushes',\n",
       " 'Further',\n",
       " 'into',\n",
       " 'Mobile',\n",
       " 'Space',\n",
       " '(',\n",
       " 'NewsFa',\n",
       " '...',\n",
       " 'Dominating',\n",
       " 'Dirrell',\n",
       " 'dispatches',\n",
       " 'Despaigne',\n",
       " '-',\n",
       " 'Leon',\n",
       " '...',\n",
       " 'Honeywell',\n",
       " 'Agrees',\n",
       " 'Takeover',\n",
       " 'of',\n",
       " 'UK',\n",
       " '#',\n",
       " '39',\n",
       " ';',\n",
       " 's',\n",
       " 'Novar',\n",
       " '-',\n",
       " '...',\n",
       " 'Lord',\n",
       " 'James',\n",
       " 'Hanson',\n",
       " ',',\n",
       " 'prominent',\n",
       " 'British',\n",
       " 'businessm',\n",
       " '...',\n",
       " 'Gallacher',\n",
       " '#',\n",
       " '39',\n",
       " ';',\n",
       " 's',\n",
       " 'wedge',\n",
       " 'shot',\n",
       " 'wraps',\n",
       " 'up',\n",
       " '#',\n",
       " '39',\n",
       " ';',\n",
       " 'dream',\n",
       " '...']"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = nltk.word_tokenize(news)\n",
    "all_words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202028"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total words in the train dataset without cleaning\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "#lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform series of cleaning steps on 'all_words'. Remove the stopwords and non-alpha characters. Also convert it to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_words = [word.lower() for word in all_words if word not in stopwords.words('english') and\n",
    "                 word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the filtered words to FrequencyDistribution chart and use the top 10000 words for features list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_freq = nltk.FreqDist(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_features = [freq[0] for freq in all_words_freq.most_common(n=10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As the features list is huge, we will save it for future purposes if required\n",
    "save_word_features = open(\"pickled_files/word_features.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    \"\"\"Finding feature set in any strings. This requires list of all words (word_features)\"\"\"\n",
    "    words = nltk.word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally generate the features set for training dataset\n",
    "documents = list(df.to_records(index=False))\n",
    "featuresets = [(find_features(rev.lower()), category) for (category, rev) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the feature set as pickle file for future purposes\n",
    "save_featuresets = open(\"./pickled_files/featureset.pickle\",\"wb\")\n",
    "pickle.dump(featuresets, save_featuresets)\n",
    "save_featuresets.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected 20000 features in random as our input. Now we will split that into training and testing features to calculate the accuracy of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing split\n",
    "testing_set = featuresets[5000:]\n",
    "training_set = featuresets[:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a list of different algorithms from sklearn library.\n",
    "\n",
    "1. Naive Bayes\n",
    "2. Multinomial Naive Bayes\n",
    "3. Bernoulli Naive Bayes\n",
    "4. Logistic Regression\n",
    "5. LinearSVC\n",
    "6. SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 90.6133333333\n",
      "Most Informative Features\n",
      "                  prices = True                3 : 2      =    282.4 : 1.0\n",
      "                internet = True                4 : 2      =    205.6 : 1.0\n",
      "                  league = True                2 : 4      =    190.0 : 1.0\n",
      "                software = True                4 : 1      =    157.2 : 1.0\n",
      "                   iraqi = True                1 : 2      =    155.6 : 1.0\n",
      "                   coach = True                2 : 3      =    151.7 : 1.0\n",
      "              technology = True                4 : 2      =    149.3 : 1.0\n",
      "                minister = True                1 : 2      =    147.8 : 1.0\n",
      "                  market = True                3 : 2      =    143.7 : 1.0\n",
      "                military = True                1 : 2      =    137.8 : 1.0\n",
      "                 olympic = True                2 : 3      =    129.5 : 1.0\n",
      "                     web = True                4 : 2      =    125.3 : 1.0\n",
      "                     oil = True                3 : 2      =    123.6 : 1.0\n",
      "                   users = True                4 : 1      =    123.5 : 1.0\n",
      "                    http = True                3 : 2      =    117.3 : 1.0\n",
      "MNB_classifier accuracy percent: 90.32\n",
      "BernoulliNB_classifier accuracy percent: 90.28\n",
      "LogisticRegression_classifier accuracy percent: 94.7533333333\n",
      "LinearSVC_classifier accuracy percent: 94.9533333333\n",
      "SGDClassifier accuracy percent: 92.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "save_classifier = open(\"./pickled_files/originalnaivebayes5k.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"./pickled_files/MNB_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"./pickled_files/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"./pickled_files/LogisticRegression_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"./pickled_files/LinearSVC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)\n",
    "\n",
    "save_classifier = open(\"./pickled_files/SGDC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(SGDC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We get good accuracy scores from different algorithms. As a next step we will create a ensemble approach with all these classifiers.\n",
    "\n",
    "Creating a class to get the majority votes from all classifiers and create a confidence interval for these classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            #print(v)\n",
    "            votes.append(v)\n",
    "            try:\n",
    "                final =  mode(votes)\n",
    "            except:\n",
    "                final = votes[0]\n",
    "        return final\n",
    "                \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a instance of above class and create the final classification for each news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(\n",
    "                                  classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "\n",
    "def sentiment(text):\n",
    "    \"\"\"This classify the news by combining all the features\"\"\"\n",
    "    feats = find_features(text)\n",
    "    return voted_classifier.classify(feats)#,voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will validate the model against test file. We will find the news which has been misclassified and calculate the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_testing = [(find_features(rev.lower()), category) for (category, rev) in doc]\n",
    "content=[]\n",
    "for d in doc:\n",
    "    content.append((sentiment(d[1]),d[0],d[1]))\n",
    "results = pd.DataFrame(content,columns=['Predicted','Actual','News'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the features testing list is huge, we will save it for future purposes if required\n",
    "save_featuresets_testing = open(\"pickled_files/featuresets_testing.pickle\",\"wb\")\n",
    "pickle.dump(featuresets_testing, save_featuresets_testing)\n",
    "save_featuresets_testing.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of news which were actually mis-classified. But this classification is the result of ensemble approach. We will also calculate the accuracy score of each individual algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Teenage T. rex's monster growth - Tyrannosauru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Storage, servers bruise HP earnings - update E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>IBM to hire even more new workers - By the end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Rivals Try to Turn Tables on Charles Schwab - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted  Actual                                               News\n",
       "3           2       4  Prediction Unit Helps Forecast Wildfires (AP) ...\n",
       "15          3       4  Teenage T. rex's monster growth - Tyrannosauru...\n",
       "19          3       4  Storage, servers bruise HP earnings - update E...\n",
       "20          3       4  IBM to hire even more new workers - By the end...\n",
       "24          3       4  Rivals Try to Turn Tables on Charles Schwab - ..."
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['Predicted']!=results['Actual']].head()#.count()/7600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ensemble mode:82.97368421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the ensemble mode: {}\".format(100-(results[results['Predicted']!=results['Actual']].count()[1]/df_test.count()[1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 88.1710526316\n",
      "MNB_classifier accuracy percent: 88.0921052632\n",
      "BernoulliNB_classifier accuracy percent: 88.0131578947\n",
      "LogisticRegression_classifier accuracy percent: 87.7368421053\n",
      "LinearSVC_classifier accuracy percent: 85.2763157895\n",
      "SGDClassifier accuracy percent: 85.6052631579\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, featuresets_testing))*100)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, featuresets_testing))*100)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, featuresets_testing))*100)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, featuresets_testing))*100)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, featuresets_testing))*100)\n",
    "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, featuresets_testing)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "Below are some of the findings from this assignment:\n",
    "1. Integrating NLTK and SKLearn library brings wider access to different algorithms.\n",
    "2. Performing data cleaning on original yields better results. Like removing stopwords, symbols.\n",
    "3. Creating ensemble models approach will provide better confidence on the predictions. If each model gives different results then we can take out the list for further investigation.\n",
    "4. Multiple models accuracy score is around 83%\n",
    "5. Each individual algorithms provides better results than multiple models approach.\n",
    "\n",
    "### Future Steps which can be performed:\n",
    "1. Create a word to vec approach and test models using that approach.\n",
    "2. Creating a convolutional neural net and perform classification using it.\n",
    "\n",
    "### References:\n",
    "1. pythonprogramming.net for combining different models.\n",
    "2. Natural Language Processing with Python book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
