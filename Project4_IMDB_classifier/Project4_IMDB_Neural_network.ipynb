{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Using the movie review document classifier discussed in this chapter, generate a classifier to classify the sentiment of the reviews. Each review can have positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "For this classification project I have downloaded [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment). As part of this project, we will be loading and clearning the dataset. Create a Convolutional DeepNeural Network(CNN) using word embedding to predict the sentiment of the each reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an initail step, we will import all the necessary packages. As we are going to use neural network, we will be using Keras with tensorflow as backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "#from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import glob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset is huge, we will download the data into separate folder and load it to the variables. Dataset has positive and negative reviews with train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset files\n",
    "train_p_files = glob.glob(\"./data/train/pos/*.txt\")\n",
    "train_n_files = glob.glob(\"./data/train/neg/*.txt\")\n",
    "test_p_files = glob.glob(\"./data/test/pos/*.txt\")\n",
    "test_n_files = glob.glob(\"./data/test/neg/*.txt\")\n",
    "train_files = [train_p_files,train_n_files]\n",
    "test_files=[test_p_files,test_n_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Xtrain and y train variables. Load positive reviews as 1 and negative reviews as  0 \n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for file in train_files:\n",
    "    for txt in file:\n",
    "        with open(txt,encoding='utf8') as f: \n",
    "\n",
    "            # positive reviews\n",
    "            if txt.find('train') !=-1 & txt.find('pos')!=-1:\n",
    "                #print(txt.find('train') & txt.find('pos'))\n",
    "                X_train.append(f.readlines())\n",
    "                y_train.append(1)\n",
    "            \n",
    "            # Negative reviews\n",
    "            elif txt.find('train') !=-1 & txt.find('neg')!=-1:\n",
    "                #print(txt.find('train') & txt.find('neg'))\n",
    "                X_train.append(f.readlines())\n",
    "                y_train.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test and y_test variables. Load positive reviews as 1 and negative reviews as  0 \n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for file in test_files:\n",
    "    for txt in file:\n",
    "        with open(txt,encoding='utf8') as f: \n",
    "\n",
    "            #print(txt)\n",
    "            if txt.find('test') !=-1 & txt.find('pos')!=-1:\n",
    "                #print(txt.find('test') & txt.find('pos'))\n",
    "                X_test.append(f.readlines())\n",
    "                y_test.append(1)\n",
    "            elif txt.find('test') !=-1 & txt.find('neg') !=-1:\n",
    "                #print(txt.find('test') & txt.find('neg'))\n",
    "                X_test.append(f.readlines())\n",
    "                y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\"]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample test data\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above dataset is loaded into variables, we need to preprocess the dataset. We need to create a word embedding for the most frequent words and use the word embedding dimension input to our deep neural network.\n",
    "\n",
    "As a first step, need to remove the symbols and convert to lower case. Below list comprehension performs the operation and converts back to sentenses for Train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_train_txt = []\n",
    "parsed_test_txt = []\n",
    "\n",
    "# Text_to_word_sequence removes symbols, changes to lowercase and tokenizes the text.\n",
    "\n",
    "parsed_train_txt = [' '.join(text_to_word_sequence(X[0])) for X in X_train] \n",
    "\n",
    "parsed_test_txt = [' '.join(text_to_word_sequence(X[0])) for X in X_test] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the unwanted symboles are removed, we need to convert the text to numbers. The way we are going to perform is follow below steps\n",
    "\n",
    "1. Initilize a tokenizer with maximum word length. \n",
    "2. Word length will be the top frequently used words in the complete dataset. In our case we have choosen as 6000.\n",
    "3. Apply Fit and text_to_sequence method on the train and test dataset.\n",
    "4. Output of the step3 will be a vector which will have index of frequently used word in that document.\n",
    "5. To standardize the input, we need to convert all the documents to a common dimenstion. So we need to apply pad_sequences to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform tokenizing as mentioned above.\n",
    "\n",
    "tokenizer = Tokenizer(6000)\n",
    "\n",
    "#Fit on the train data\n",
    "tokenizer.fit_on_texts(parsed_train_txt)\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "#Apply it to train and test dataset\n",
    "X_train_txt = tokenizer.texts_to_sequences(parsed_train_txt)\n",
    "X_test_txt = tokenizer.texts_to_sequences(parsed_test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To standardize the input data, convert it to a common length. Here we have choosen as 500\n",
    "\n",
    "X_train = pad_sequences(X_train_txt,maxlen=500)\n",
    "X_test = pad_sequences(X_test_txt,maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "        309,    6,    3, 1069,  209,    9, 2161,   30,    1,  169,   55,\n",
       "         14,   46,   82, 5844,   41,  392,  110,  138,   14, 5340,   58,\n",
       "       4449,  150,    8,    1, 4988, 5924,  482,   69,    5,  261,   12,\n",
       "       2002,    6,   73, 2425,    5,  632,   71,    6, 5340,    1,    5,\n",
       "       2003,    1, 5925, 1534,   34,   67,   64,  205,  140,   65, 1230,\n",
       "          1,    4,    1,  223,  901,   29, 3022,   69,    4,    1, 5845,\n",
       "         10,  693,    2,   65, 1534,   51,   10,  216,    1,  387,    8,\n",
       "         60,    3, 1467, 3712,  800,    5, 3513,  177,    1,  392,   10,\n",
       "       1237,   30,  309,    3,  353,  344, 2974,  143,  130,    5,   28,\n",
       "          4,  126, 5340, 1467, 2373,    5,  309,   10,  532,   12,  108,\n",
       "       1468,    4,   58,  555,  101,   12,  309,    6,  227, 4174,   48,\n",
       "          3, 2232,   12,    9,  215])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data after pad_sequences\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have completed all the preprocessing steps. Next is to create a deep neural network with below layers.\n",
    "\n",
    "1. As this is a sequential problem, we will create a sequential model using Keras.\n",
    "2. Add a word embedding layer to the model. Here the vector size will be 6000. This is the total number of words which will be considered. Other parameters are dimension and document input length.\n",
    "3. Add a 1 Dimensional convolution layer with a keranl size of 3. We will have same padding with relu activation layer. This will reduce the dimension and gather important features in the dataset.\n",
    "4. As a next step, we will apply Max Pooling 1D layer, this will again reduce the dimensions of the data and will get only the important features in the dataset.\n",
    "5. We are now done with the convolutional layer. Now we will need to flatten the dimension and create a fully connectioned network. \n",
    "6. Add two more hidden layers with 500 and 250 perceptron to make it more accurate.\n",
    "\n",
    "Finally compile the model with RMSPROP optimizer and binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 32)           192000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 500)               4000500   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,321,105\n",
      "Trainable params: 4,321,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Sequential model\n",
    "model = Sequential()\n",
    "# 6000 voc. size, 32 dimension, 500 length\n",
    "model.add(Embedding(6000,32,input_length=500))\n",
    "\n",
    "# Add Convolution 1D layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "# Add Maxpooling 1D layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation='relu'))\n",
    "#Add two hidden layers\n",
    "model.add(Dense(250,activation='relu'))\n",
    "\n",
    "#Finally predict the output using sigmoid \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#Compile the model\n",
    "model.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fully created, we need to fit the model with training data and validated using testing data. Here the batch size will be 128 and epochs will be 10(depending on the pc specs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 87s 3ms/step - loss: 0.4680 - acc: 0.7435 - val_loss: 0.5428 - val_acc: 0.7516\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.2344 - acc: 0.9069 - val_loss: 0.5283 - val_acc: 0.8058\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.1742 - acc: 0.9330 - val_loss: 0.3075 - val_acc: 0.8910\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.1218 - acc: 0.9558 - val_loss: 0.3248 - val_acc: 0.8844\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.0739 - acc: 0.9738 - val_loss: 0.4121 - val_acc: 0.8831\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.0497 - acc: 0.9856 - val_loss: 0.5711 - val_acc: 0.8746\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.0306 - acc: 0.9930 - val_loss: 0.8113 - val_acc: 0.8627\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.0306 - acc: 0.9943 - val_loss: 0.7557 - val_acc: 0.8734\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.0219 - acc: 0.9967 - val_loss: 0.9716 - val_acc: 0.8713\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.0062 - acc: 0.9992 - val_loss: 1.2190 - val_acc: 0.8688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46116a20>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fitting\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=128,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sentiment of test dataset using predict function of keras. Here predict will provide the probability if we need to get more details. For now we will use predict_classes method which wil provide the final sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accracy of the model.\n",
    "print(\"Accuray of the model: {}\".format(model.evaluate(X_test,y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 26s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report of test dataset: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11256,  1244],\n",
       "       [ 2036, 10464]], dtype=int64)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a confusion matrix using sklearn classification method.\n",
    "\n",
    "print(\"Classification report of test dataset: \\n\")\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the weights are defined for the each layer. For all the 6000 frequently used words, some weight has bee assigned. Below are the diferent shapes of the model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 32)\n",
      "(3, 32, 32)\n",
      "(32,)\n",
      "(8000, 500)\n",
      "(500,)\n",
      "(500, 250)\n",
      "(250,)\n",
      "(250, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for w in model.get_weights():\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "1. We have successfully preprecessed the text data and converted into a vector format using word embeddings.\n",
    "2. We have created a deep neural network with convolution and maxpooling to get the important features in the dataset.\n",
    "3. Created a fully connected layers with hidden layers to predict the sentiment of the reviews.\n",
    "4. Model gives a accuracy of around 87%. Which is good for this type of dataset.\n",
    "5. Accuracy can be increased by adding more hidden layers and tuning the hyper parameters. Also pre-trained word embedding can be used get get higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
